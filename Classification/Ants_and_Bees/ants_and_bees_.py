# -*- coding: utf-8 -*-
"""Ants and Bees .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/109JWdmgIQmbX4gP4_B_4BHf4Lo5TEXDm
"""

# Commented out IPython magic to ensure Python compatibility.
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import numpy as np
from torchvision import datasets,transforms, models
import torch.nn.functional as F
# %matplotlib inline

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

!git clone https://github.com/jaddoescad/ants_and_bees.git

!ls

!ls ants_and_bees/

!ls ants_and_bees/train

!ls ants_and_bees/train/ants

train_transform = transforms.Compose([transforms.Resize((224,224)),
                                transforms.RandomHorizontalFlip(),
                                transforms.RandomRotation(10),
                                transforms.ColorJitter(contrast=0.8,brightness=0.8,saturation =0.8),
                                transforms.ToTensor(),
                                transforms.Normalize((0.5,),(0.5,))])


transform = transforms.Compose([transforms.Resize((224,224)),
                                transforms.ToTensor(),
                               transforms.Normalize((0.5,),(0.5,))])
training_data = datasets.ImageFolder('ants_and_bees/train',transform=train_transform)
train_loader = torch.utils.data.DataLoader(dataset=training_data,batch_size=20,shuffle=True)
validation_data = datasets.ImageFolder('ants_and_bees/val',transform=transform)

validation_loader = torch.utils.data.DataLoader(dataset = validation_data,shuffle=False,batch_size=20)

print(len(training_data))
print(len(validation_data))

def im_convert(tensor):
    image = tensor.cpu().clone().detach().numpy()
    image = image.transpose(1,2,0)
    image = image * np.array((0.5,0.5,0.5)) + np.array((0.5,0.5,0.5))
    image = image.clip(0,1)
    return image

classes = ('Ants', 'Bees')

data_iter = iter(train_loader)
images, labels = data_iter.next()
fig = plt.figure(figsize=(25,4))
for idx in np.arange(20):
    ax = fig.add_subplot(2,10,idx+1,xticks = [],yticks=[])
    plt.imshow(im_convert(images[idx]))
    ax.set_title(classes[labels[idx].item()])

model = models.vgg16(pretrained=True)

print(model)

for param in model.features.parameters():
  param.requires_grad = False

n_inputs = model.classifier[6].in_features
last_layer =nn.Linear(n_inputs, len(classes))
model.classifier[6] = last_layer

print(model)

model.to(device)

criteria = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(),lr=0.0001)

epochs = 6
running_loss_hist = []
running_correct_hist=[]
val_correct_hist=[]
val_loss_hist = []
for i in range(epochs):
    run_loss = 0.0
    run_corr = 0.0
    val_run_loss = 0.0
    val_run_corr = 0.0
    
    for inputs,labels in train_loader:
        inputs  = inputs.to(device)
        labels = labels.to(device)
        outputs = model(inputs)
        loss = criteria(outputs,labels)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        _,pred = torch.max(outputs,1)
        run_loss += loss.item()
        run_corr += torch.sum(pred==labels)
    
    else:
        with torch.no_grad():
            for val_inputs,val_labels in validation_loader:
                val_inputs = val_inputs.to(device)
                val_labels = val_labels.to(device)
                val_outputs = model(val_inputs)
                val_loss = criteria(val_outputs,val_labels)
                
                _,val_pred = torch.max(val_outputs,1)
                val_run_loss += val_loss.item()
                val_run_corr += torch.sum(val_pred == val_labels)
                
        epoch_val_loss = val_run_loss/len(validation_loader.dataset)
        epoch_val_corr = val_run_corr.float()/len(validation_loader.dataset)
        epoch_loss = run_loss/len(train_loader.dataset)
        epoch_corr = run_corr.float()/len(train_loader.dataset)
        running_loss_hist.append(epoch_loss)
        running_correct_hist.append(epoch_corr)
        val_correct_hist.append(epoch_val_corr)
        val_loss_hist.append(epoch_val_loss)
        print('Epoch : ',i+1)
        print("Training Loss : {:.4f} , Correct : {:.4f}".format(epoch_loss,epoch_corr.item()))
        print("Validation Loss : {:.4f} , Correct : {:.4f}".format(epoch_val_loss,epoch_val_corr.item()))

plt.plot(running_loss_hist,label='Training Loss')
plt.plot(val_loss_hist,label = 'Val loss')
plt.legend()

plt.plot(running_correct_hist,label='Training Correct')
plt.plot(val_correct_hist,label = 'VAl correct')
plt.legend()

import requests
import PIL.ImageOps
from PIL import Image

url = 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRZDCzyqsEK757VHn7jEkf9y8OyTXYlCxdIwFHN-FXTttNVsHE6&s'
response = requests.get(url,stream = True)
img = Image.open(response.raw)
plt.imshow(img)

img = transform(img)
plt.imshow(im_convert(img))

img.shape

img = img.to(device)
img = img.unsqueeze(0)
output = model(img)
_,pred = torch.max(output,1)
print(classes[pred.item()])

data_iter = iter(validation_loader)
images, labels = data_iter.next()
images = images.to(device)
labels = labels.to(device)

output = model(images)
_,preds = torch.max(output,1)

fig = plt.figure(figsize=(25,4))
for idx in np.arange(20):
    ax = fig.add_subplot(2, 10, idx+1, xticks=[], yticks=[])
    plt.imshow(im_convert(images[idx]))
    ax.set_title("{} ({})".format(str(classes[preds[idx].item()]), str(classes[labels[idx].item()])), color=("green" if preds[idx]==labels[idx] else "red"))

"""Now With AlexNet"""



model = models.alexnet(pretrained = True)
print(model)

for param in model.features.parameters():
  param.requires_grad_(False)

n_inputs = model.classifier[6].in_features
print(n_inputs)

last_layer = nn.Linear(n_inputs,2)
model.classifier[6] = last_layer
print(model)

model = model.to(device)

criteria_alex = nn.CrossEntropyLoss()
optimizer_alex = torch.optim.Adam(model.parameters(),lr=0.0001)

epochs = 6
running_loss_hist = []
running_correct_hist=[]
val_correct_hist=[]
val_loss_hist = []
for i in range(epochs):
    run_loss = 0.0
    run_corr = 0.0
    val_run_loss = 0.0
    val_run_corr = 0.0
    
    for inputs,labels in train_loader:
        inputs  = inputs.to(device)
        labels = labels.to(device)
        outputs = model(inputs)
        loss = criteria_alex(outputs,labels)
        optimizer_alex.zero_grad()
        loss.backward()
        optimizer_alex.step()
        
        _,pred = torch.max(outputs,1)
        run_loss += loss.item()
        run_corr += torch.sum(pred==labels)
    
    else:
        with torch.no_grad():
            for val_inputs,val_labels in validation_loader:
                val_inputs = val_inputs.to(device)
                val_labels = val_labels.to(device)
                val_outputs = model(val_inputs)
                val_loss = criteria_alex(val_outputs,val_labels)
                
                _,val_pred = torch.max(val_outputs,1)
                val_run_loss += val_loss.item()
                val_run_corr += torch.sum(val_pred == val_labels)
                
        epoch_val_loss = val_run_loss/len(validation_loader.dataset)
        epoch_val_corr = val_run_corr.float()/len(validation_loader.dataset)
        epoch_loss = run_loss/len(train_loader.dataset)
        epoch_corr = run_corr.float()/len(train_loader.dataset)
        running_loss_hist.append(epoch_loss)
        running_correct_hist.append(epoch_corr)
        val_correct_hist.append(epoch_val_corr)
        val_loss_hist.append(epoch_val_loss)
        print('Epoch : ',i+1)
        print("Training Loss : {:.4f} , Correct : {:.4f}".format(epoch_loss,epoch_corr.item()))
        print("Validation Loss : {:.4f} , Correct : {:.4f}".format(epoch_val_loss,epoch_val_corr.item()))

plt.plot(running_loss_hist,label='Training Loss')
plt.plot(val_loss_hist,label = 'Val loss')
plt.legend()

plt.plot(running_correct_hist,label='Training Correct')
plt.plot(val_correct_hist,label = 'VAl correct')
plt.legend()

url = 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRZDCzyqsEK757VHn7jEkf9y8OyTXYlCxdIwFHN-FXTttNVsHE6&s'
response = requests.get(url,stream = True)
img = Image.open(response.raw)
plt.imshow(img)

img = transform(img)
plt.imshow(im_convert(img))

img = img.to(device)
img = img.unsqueeze(0)
output = model(img)
_,pred = torch.max(output,1)
print(classes[pred.item()])

data_iter = iter(validation_loader)
images, labels = data_iter.next()
images = images.to(device)
labels = labels.to(device)

output = model(images)
_,preds = torch.max(output,1)

fig = plt.figure(figsize=(25,4))
for idx in np.arange(20):
    ax = fig.add_subplot(2, 10, idx+1, xticks=[], yticks=[])
    plt.imshow(im_convert(images[idx]))
    ax.set_title("{} ({})".format(str(classes[preds[idx].item()]), str(classes[labels[idx].item()])), color=("green" if preds[idx]==labels[idx] else "red"))